import os
import requests

# Directory containing VLM outputs (produced by Vision-Language Model step)
vlm_dir = 'analysis_results'  # Relative to the main project directory
# Output directory for Ollama LLM analyses (this script's directory)
output_dir = 'ollama_llm_analysis'
# Name of the Ollama model to use (must be pulled and available in Ollama)
ollama_model = 'llama3'
# Ollama API endpoint (default for local Ollama server)
ollama_url = 'http://localhost:11434/api/generate'

# Find all *_vlm.txt files in the analysis_results directory
# These files should contain the basic image descriptions generated by VLM
vlm_files = [f for f in os.listdir(vlm_dir) if f.endswith('_vlm.txt')]
if not vlm_files:
    print('No VLM output files found in analysis_results/.')
    exit(0)

# Process each VLM output file
for vlm_file in vlm_files:
    vlm_path = os.path.join(vlm_dir, vlm_file)
    with open(vlm_path, 'r') as f:
        description = f.read()

    # Compose a strong prompt for the LLM, instructing it to focus exclusively on philosophical analysis
    prompt = (
        "You are a philosopher. Given the following image description, do not repeat or summarize the description. "
        "Instead, provide only a deep philosophical reflection on its meaning, implications, or what it reveals about the human condition. "
        "Focus exclusively on philosophical analysis, not on description or summary.\n\n"
        "Description:\n" + description
    )

    # Send the prompt to the Ollama server via its HTTP API
    # The server must be running (use `ollama serve` in a terminal)
    response = requests.post(
        ollama_url,
        json={
            'model': ollama_model,  # Model name as recognized by Ollama
            'prompt': prompt,       # The prompt to send
            'stream': False         # We want the full response, not a stream
        }
    )
    # Check if the request was successful
    if response.status_code == 200:
        # Extract the generated philosophical analysis from the response
        result = response.json()['response']
        # Name the output file based on the input file
        out_name = vlm_file.replace('_vlm.txt', '_philosophy_ollama.txt')
        output_path = os.path.join(output_dir, out_name)
        # Save the result to a text file in the output directory
        with open(output_path, 'w') as f:
            f.write(result)
        print(f"Philosophical analysis saved to {output_path}")
    else:
        # Print an error message if the Ollama API call failed
        print(f"Error from Ollama for {vlm_file}: {response.text}") 
